{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Assignment7_Solutions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FabriceBeaumont/4216_Biomedical_DS_and_AI/blob/main/Sheet7/Assignment7_Solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UnHoHsFop-8"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "import random as rand"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUswbzmRCHbj"
      },
      "source": [
        "def get_dataset_from_github(filename, index_col_str=None, header_str='infer'):    \n",
        "    data_file_path = \"https://raw.githubusercontent.com/D34dP0oL/4216_Biomedical_DS_and_AI/main/Datasets/\"\n",
        "    if index_col_str is None and header_str == 'infer':\n",
        "      data = pd.read_csv(data_file_path + filename)\n",
        "    elif index_col_str is None:\n",
        "        data = pd.read_csv(data_file_path + filename, header=header_str)\n",
        "    elif header_str == 'infer':\n",
        "      data = pd.read_csv(data_file_path + filename, index_col=index_col_str)\n",
        "    else:\n",
        "      data = pd.read_csv(data_file_path + filename, index_col=index_col_str, header=header_str)\n",
        "\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx0G72bGl7xc"
      },
      "source": [
        "## Biomedical Data Science & AI\n",
        "\n",
        "## Assignment 7\n",
        "\n",
        "#### Group members:  Fabrice Beaumont, Fatemeh Salehi, Genivika Mann, Helia Salimi, Jonah"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fhYMe5ml7xo"
      },
      "source": [
        "---\n",
        "### Exercise 1 - Elastic Net & Nested Cross-Validation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgUkN-nGziCe"
      },
      "source": [
        "#### 1.1. Using the `titanic_survival_data.csv` dataset, train a logistic regression model with elastic net penalization to demonstrate the pros and cons of the different data splitting methods and give a short description on what you observe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWG_h9C_o2bZ"
      },
      "source": [
        "##### 1.1.a) Report the accuracy of data splitting with a test size of $0.2$ and random state as $1$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHb-R-M0o4d4"
      },
      "source": [
        "##### 1.1.b) Plot the boxplot for the accuracy of the **$K$-fold cross validation** with $5$ splits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKQMVXZLo6A1"
      },
      "source": [
        "##### 1.1.c) Plot the boxplot for the accuracy of the **Stratified-$K$-fold cross validation** with $5$ splits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkWfRRu6o7HE"
      },
      "source": [
        "##### 1.1.d) Inform yourself about **leave-one-out cross-validation** (**LOOCV**). Implement LOOCV and mention the pros and cons of the method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "appoDszfFI5l"
      },
      "source": [
        "#### 1.2. Use the nested cross validation to train a logistic regression with elastic net penalization (`leukemia_small.csv`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhfmrS-AFOra"
      },
      "source": [
        "##### 1.2.a) Split the data into training and test samples using an appropriate cross validation method, and in the inner loop carry out **hyperparameter optimization**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CJhiLbSpoOm"
      },
      "source": [
        "##### 1.2.b) Compute the area under the ROC curve (**AUC-ROC**) and the area under the precision-recall curve (**AUC-PR**)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r290kRylppoH"
      },
      "source": [
        "##### 1.2.c) Plot separate boxplots for the two performance metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NExXYSYPFL78"
      },
      "source": [
        "#### 1.3. In your own words, explain how each of the following metrics can be used to assess the performance of a model and then calculate each metric using the following confusion matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB9TtcYdqJZH"
      },
      "source": [
        " _           | Predicted No | Predicted Yes |\n",
        "---|---|---\n",
        "Actual No    | $250$        | $20$          |\n",
        "Actual Yes   | $30$         | $100$         |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnWW-nzRFNGQ"
      },
      "source": [
        "##### 1.3.a) Recall\n",
        "\n",
        "With recall we can measure what percentage of the total positives are predicted to be positive, so in other words, it gives us a measure of the true positive rate.\n",
        "\n",
        "Calculation:\n",
        "\n",
        "$Recall = \\frac{TP}{TP+FN} = \\frac{100}{100+30} \\approx 77\\%$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11_ebbYlp8cS"
      },
      "source": [
        "##### 1.3.b) $F_1$\n",
        "\n",
        "The F1-Score measures the balance between precision and recall. While the recall measures how many false negatives we have, the precision give us an indication of the number of false positives. If the model has high recall and precision this leads to a high F1-Score. The F1-Score is especially useful as a performance measure if we have an uneven class distribution.\n",
        "\n",
        "Calculation:\n",
        "\n",
        "$Precision = \\frac{TP}{TP+FP} = \\frac{100}{100+20} \\approx 83\\%$\n",
        "\n",
        "$F1 = 2\\cdot \\frac{Precision \\cdot Recall}{Precision + Recall} = 2\\cdot \\frac{0.833\\cdot 0.769}{0.833 + 0.769} \\approx 0.8$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvzK-GqDp-H-"
      },
      "source": [
        "##### 1.3.c) Balanced Accuracy (BAC)\n",
        "\n",
        "Balanced Accuracy is the arithmetic mean between recall (also called sensitivity/true positive rate in this scope) and specificity. The specificity is a measure for the true negative rate. Like the F1-Score the balanced accuracy is especially useful to measure the performance of a model when the classes are imbalanced as it attempts to account for the imbalance in classes.\n",
        "\n",
        "Calculation:\n",
        "\n",
        "$Specificity = \\frac{TN}{TN+FP} = \\frac{250}{250+20} \\approx 93\\%$\n",
        "\n",
        "$BAC = \\frac{TPR + TNR}{2} = \\frac{0.769 + 0.926}{2} \\approx 0.85$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gpr7R7PRp_KB"
      },
      "source": [
        "##### 1.3.d) Matthews Correlation Coefficient (MCC)\n",
        "\n",
        "Matthew Correlation Coefficient gives us a measure of the differences between the real values and the predicted values. The difference takes true positives, false positives, true negatives and false negatives into account and returns a high score only if for all four measures the model has good results.\n",
        "\n",
        "Calculation:\n",
        "\n",
        "$MCC = \\frac{TP\\cdot TN - FP\\cdot FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}} = \\frac{100\\cdot 250 - 20\\cdot 30}{\\sqrt{(100+20)(100+30)(250+20)(250+30)}} \\approx 0.71$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v3MrpiqElLz"
      },
      "source": [
        "---\n",
        "### Exercise 2 - SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsbQAaMbY7q1"
      },
      "source": [
        "#### 2.1. Inform yourself about **SVM** and briefly explain the working strategy of linear SVM and why maximizing the margin is a good strategy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zy3FrdNcDPqR"
      },
      "source": [
        "SVM is a *supervised machine learning algorithm* which can be used for \n",
        "- classification or \n",
        "- regression problems. \n",
        "\n",
        "It uses a technique called the *kernel trick* to transform your data and then based on these transformations it finds an optimal boundary between the possible outputs. Simply put, it does some extremely complex data transformations, then figures out how to seperate your data based on the labels or outputs you have defined.\n",
        "\n",
        "A simple linear SVM classifier works by making a straight line between two classes. That means all of the data points on one side of the line will represent a category and the data points on the other side of the line will be put into a different category. This means there can be an infinite number of lines to choose from.\n",
        "\n",
        "What makes the linear SVM algorithm better than some of the other algorithms, like $k$-nearest neighbors, is that it chooses the best line to classify your data points. It chooses the line that separates the data and is the furthest away from the closet data points as possible.\n",
        "\n",
        "A large margin effectively corresponds to a regularization of SVM weights which prevents overfitting. Hence, we prefer a large margin (or the right margin chosen by cross-validation) because it helps us generalize our predictions and perform better on the test data by not overfitting the model to the training data.\n",
        "\n",
        "The intuition as that decision boundary that maximises the margin would be the most useful, as they create the most separation between boundary cases so that small variations will be less likely to affect the classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FT2Xl_AE1Fk"
      },
      "source": [
        "#### 2.2. Inform yourself about the non-linearity problem for classifiers. Briefly explain how SVM uses **kernel trick** to overcome this issue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kITgL-LfDy9C"
      },
      "source": [
        "If the data are not linearly separable, a linear classification cannot perfectly distinguish the two classes. Nonlinear functions can be used to separate instances that are not linearly separable.\n",
        "\n",
        "In machine learning, a trick known as **kernel trick** is used to learn a linear classifier to classify a non-linear dataset. It transforms the linearly inseparable data into a linearly separable one by projecting it into a higher dimension. A kernel function is applied on each data instance to map the original non-linear data points into some higher dimensional space in which they become linearly separable.\n",
        "\n",
        "To get a better understanding, let’s consider circles dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPOToyFTETq4"
      },
      "source": [
        "# TODO: 1.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlf04T63D-NN"
      },
      "source": [
        "The dataset is clearly a non-linear dataset and consists of two features (say, $X$ and $Y$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcNg1mniECIx"
      },
      "source": [
        "In order to use SVM for classifying this data, introduce another feature $Z = X^2 + Y^2$ into the dataset. Thus, projecting the 2-dimensional data into 3-dimensional space. The first dimension representing the feature $X$, second representing $Y$ and third representing $Z$ (which, mathematically, is equal to the radius of the circle of which the point $(x, y)$ is a part of). Now, clearly, for the data shown above, the *yellow* data points belong to a circle of smaller radius and the *purple* data points belong to a circle of larger radius. Thus, the data becomes linearly separable along the $Z$-axis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwKiYcNHEV4Z"
      },
      "source": [
        "# TODO: 2.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W8jwBlpE9z7"
      },
      "source": [
        "---\n",
        "### Exercise 3 - Random Forest\n",
        "\n",
        "For the following questions, use `random_seed = 1` for better reproducibility of your\n",
        "answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tg4ZR1dOE_gU"
      },
      "source": [
        "#### 3.1. Load the breast cancer dataset from sklearn to your Jupyter notebook. Use label encoding to convert your target variable “class” into numerical form. Split the dataset using a $5$-fold cross validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UswnbGnvFDY5"
      },
      "source": [
        "#### 3.2. Set up a parameter grid and use grid search with $5$-fold cross validation to identify the best hyperparameter values used to fit a random forest classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_REmdztsGyT"
      },
      "source": [
        "#### 3.3. Use the best hyperparameters from *2)* to fit the final model. Predict the classes of the test set and count the number of samples assigned to each class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2qVdnYgsH37"
      },
      "source": [
        "#### 3.4. Print the importance of each feature in descending order. Identify the top five features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bd1K747ssJBm"
      },
      "source": [
        "#### 3.5. Mention a case when permutation feature importance is favored over impurity-based feature importance. Use permutation importance to print the importances of your features in a descending order. Compare your answer with that of *4)*. Do you notice any differences?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEewS1cBsKR4"
      },
      "source": [
        "#### 3.6. In your own words, explain the **bootstrapping technique** and mention how random forest benefits from its application."
      ]
    }
  ]
}