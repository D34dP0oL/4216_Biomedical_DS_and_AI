{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Assignment9_Solutions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FabriceBeaumont/4216_Biomedical_DS_and_AI/blob/main/Sheet9/Assignment9_Solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UnHoHsFop-8"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy.random import seed\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense, Activation, Dropout\n",
        "# from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, KFold, cross_val_predict, train_test_split\n",
        "from sklearn.calibration import calibration_curve"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUswbzmRCHbj"
      },
      "source": [
        "def get_dataset_from_github(filename, index_col_str=None, header_str='infer'):    \n",
        "    data_file_path = \"https://raw.githubusercontent.com/FabriceBeaumont/4216_Biomedical_DS_and_AI/tree/main/Datasets\"\n",
        "    if index_col_str is None and header_str == 'infer':\n",
        "      data = pd.read_csv(data_file_path + filename)\n",
        "    elif index_col_str is None:\n",
        "        data = pd.read_csv(data_file_path + filename, header=header_str)\n",
        "    elif header_str == 'infer':\n",
        "      data = pd.read_csv(data_file_path + filename, index_col=index_col_str)\n",
        "    else:\n",
        "      data = pd.read_csv(data_file_path + filename, index_col=index_col_str, header=header_str)\n",
        "\n",
        "    return data"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx0G72bGl7xc"
      },
      "source": [
        "## Biomedical Data Science & AI\n",
        "\n",
        "## Assignment 9\n",
        "\n",
        "#### Group members:  Fabrice Beaumont, Fatemeh Salehi, Genivika Mann, Helia Salimi, Jonah"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fhYMe5ml7xo"
      },
      "source": [
        "---\n",
        "### Exercise 1 - Basics of NN\n",
        "\n",
        "From the MNIST database load the handwritten digits dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-WB8i9iunPs"
      },
      "source": [
        "# Load the data: \n",
        "# Split between train and test sets w.r.t. data 'x' and class vectors 'y'\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_krKhWqo-Bv7"
      },
      "source": [
        "#### 1.1. Normalize your dataset before training your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGazybua_y8x"
      },
      "source": [
        "# For normalization we scale the images to contain values in [0, 1]\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9a6HLa7AAic",
        "outputId": "31c8a1a3-c150-48fb-919b-e52af2319d7a"
      },
      "source": [
        "# Check, that all images have shape (28, 28, 1)\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "input_shape = x_train[0].shape\n",
        "\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(x_train.shape[0], \"train samples\")\n",
        "print(x_test.shape[0], \"test samples\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFf6r2oRATnf",
        "outputId": "1531f1b6-5ba6-4edf-d1d6-2ed6db064738"
      },
      "source": [
        "# Check, if the number of classes is ten (since there are ten digits 0,...,9)\n",
        "num_classes = 10\n",
        "\n",
        "num_classes == len(set(y_train)) == len(set(y_test))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZAXp3o0ADJp"
      },
      "source": [
        "# Convert the class vectors 'y' to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "appoDszfFI5l"
      },
      "source": [
        "#### 1.2. Train a neural network once using **Adam** and once using **AdaGrad** optimizer. \n",
        "\n",
        "*Hint*: Set epochs to $20$, neurons of hidden layer to $100$ and use the ReLU as activation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6VsDVuWA8_F"
      },
      "source": [
        "num_epochs = 20\n",
        "num_hidden_neurons = 100\n",
        "activation_fct = \"relu\"\n",
        "loss = \"sparse_categorical_crossentropy\"\n",
        "\n",
        "batch_size = 128\n",
        "validation_split = 0.1"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Op8wHW6YAsb5",
        "outputId": "a3a95d6a-ba69-422b-e4a4-685d8315412d"
      },
      "source": [
        "model = keras.Sequential(\n",
        "    [\n",
        "        # # Input layer\n",
        "        # keras.Input(shape=input_shape),\n",
        "     \n",
        "        # # One hidden layer\n",
        "        # layers.Dense(num_hidden_neurons, activation=activation_fct),\n",
        "\n",
        "        # # Output layer\n",
        "        # # Use 'softmax' to get a probability for the digits 0,...,9 as classification\n",
        "        # layers.Flatten(),\n",
        "        # layers.Dense(num_classes, activation=\"softmax\"),\n",
        "     \n",
        "        keras.Input(shape=input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                16010     \n",
            "=================================================================\n",
            "Total params: 34,826\n",
            "Trainable params: 34,826\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "XNjhS6ZJDD0Z",
        "outputId": "8f4756e0-82ce-46a6-a0b0-064e5e9991b0"
      },
      "source": [
        "# Train the model with 'adam'\n",
        "model.compile(loss=loss, optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "history_adam = model.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs, validation_split=validation_split)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-28d76ba9a413>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# score = model.evaluate(x_test, y_test, verbose=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m:  logits and labels must have the same first dimension, got logits shape [128,10] and labels shape [1280]\n\t [[node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits (defined at <ipython-input-14-28d76ba9a413>:4) ]] [Op:__inference_train_function_1351]\n\nFunction call stack:\ntrain_function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5udg5TmEHNIe"
      },
      "source": [
        "score_adam = model.evaluate(input_test, target_test, verbose=0)\n",
        "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFcKiI_nHO4_"
      },
      "source": [
        "# Now do the same using AdaGrad\n",
        "# Train the model with 'adam'\n",
        "model.compile(loss=loss, optimizer=\"adagrad\", metrics=[\"accuracy\"])\n",
        "\n",
        "history_adagrad = model.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs, validation_split=validation_split)\n",
        "\n",
        "score_adagrad = model.evaluate(input_test, target_test, verbose=0)\n",
        "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyw9E17lFKxz"
      },
      "source": [
        "#### 1.3. Plot the *SparseCategoricalCrossentropy* loss for both models. Plot the computed accuracy for both models. Which model performed better while training?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoMZjuceI4Y_"
      },
      "source": [
        "# First check, what information was stored in the histories\n",
        "print(history_adam.history.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSRUWsLCu9J3"
      },
      "source": [
        "# Now plot the losses\n",
        "plt.plot(history_adam.history['loss'])\n",
        "plt.plot(history_adam.history['val_loss'])\n",
        "\n",
        "plt.plot(history_adagrad.history['loss'])\n",
        "plt.plot(history_adagrad.history['val_loss'])\n",
        "\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Adam_train', 'Adam_validation', 'Adagrad_train', 'Adagrad_validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUIsXo-Yu_dH"
      },
      "source": [
        "#### 1.4. Compute the model accuracy on the test set for both optimizers. Which model performed better?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XT0URUTYvBjn"
      },
      "source": [
        "# Plot the accuracies\n",
        "plt.plot(history_adam.history['acc'])\n",
        "plt.plot(history_adam.history['val_acc'])\n",
        "\n",
        "plt.plot(history_adagrad.history['acc'])\n",
        "plt.plot(history_adagrad.history['val_acc'])\n",
        "\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Adam_train', 'Adam_validation', 'Adagrad_train', 'Adagrad_validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omo2nxYuvDZv"
      },
      "source": [
        "#### 1.5. Familiarize yourself with **Layer Normalization** and explain how it works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuNTbYvevEI_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj_QiGbovFHI"
      },
      "source": [
        "#### 1.6. Using the same dataset to train a neural network with Layer Normalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXW6NPA7vGzn"
      },
      "source": [
        "##### 1.6.a. Compute the SparseCategoricalCrossentropy loss and model accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CzB1JOcvI5J"
      },
      "source": [
        "##### 1.6.b. Evaluate the model performance using the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dLlCzPpvFyJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwJ9lFCMvL3i"
      },
      "source": [
        "---\n",
        "### Exercise 2 - Hyper Parameter Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFdF9PLyvOyT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qALaDAgvQ9h"
      },
      "source": [
        "#### 2.1. What are the main challenges with hyper-parameter optimization for neural networks?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAW6mdQlvWBl"
      },
      "source": [
        "##### 2.2. Inform yourself about variants of **Bayesian-HPO** and explain them in detail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXORgwslvaQ2"
      },
      "source": [
        "#### 2.3. Using the same MNIST dataset, optimize the activation function for the output layer and the number of dropout units in the NN model using the following methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaY0eyycve4w"
      },
      "source": [
        "##### 2.3.a. Grid search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUOwcULPvgxh"
      },
      "source": [
        "##### 2.3.a. Random search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGjTMl4tviNM"
      },
      "source": [
        "##### 2.3.a. Bayesian Hyper-parameter optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UF0_fP2vQwH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0BD_oB8vmTJ"
      },
      "source": [
        "---\n",
        "### Exercise 3 - Transfer Learning & CNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3az3ObKlvp-B"
      },
      "source": [
        "#### 3.1. Load the *VGG16 pre-trained model* using Keras Applications API. Use the model to classify the dog images in canines.zip after pre-processing each image by doing the following:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqepkwhKvu-6"
      },
      "source": [
        "##### 3.1.a. Load each image and set the size to 224 x 224 pixels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ttioWE5vySg"
      },
      "source": [
        "##### 3.1.b. Convert the image pixels to a numpy array and reshape it according to the model’s input requirements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "op0xzm-nv1G0"
      },
      "source": [
        "##### 3.1.c. Use the model to print out the predicted class and its probability for each image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StTqwf0Vv6Q2"
      },
      "source": [
        "#### 3.2. Downscale the given matrix by applying the following pooling operations:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALJ8BJ0rv9SO"
      },
      "source": [
        "##### 3.2.a. Max Pool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glyGSHsev_Yk"
      },
      "source": [
        "##### 3.2.a. Average Pool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxTmNGWHwBSl"
      },
      "source": [
        "#### 3.3. Load the **CIFAR10 dataset** using Keras datasets API and normalize the images’ pixel values. Train a convolutional neural network to classify the dataset images with the following architecture:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Toy-Aut09UFp"
      },
      "source": [
        "##### 3.3.a. Convolutional Base:\n",
        "1. An input convolution layer with $32$ filters and a kernel size of $(3,3)$.\n",
        "Adjust your input shape to that of the CIFAR images’ format\n",
        "2. Two convolution layers, each with 64 filters and a kernel size of $(3,3)$\n",
        "3. Two Max Pool layers, with a pool size of $2\\times 2$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-JwLlof9kwa"
      },
      "source": [
        "##### 3.3.b. Two dense layers, with $64$ and $10$ units respectively. Adjust the output of the convolutional base such that it satisfies the input requirements of the dense layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mUj_ufp9qXr"
      },
      "source": [
        "##### 3.3.c. Use the following parameters to train the network:\n",
        "1. Sparse categorical cross entropy as your loss function\n",
        "1. Adam optimizer\n",
        "1. $10$ epochs\n",
        "1. ReLU activation for your layers\n",
        "\n",
        "Compile your model, then plot the accuracy across each epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVTrdDV8voV-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}