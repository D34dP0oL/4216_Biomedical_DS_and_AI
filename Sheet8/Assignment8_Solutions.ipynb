{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Assignment8_Solutions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FabriceBeaumont/4216_Biomedical_DS_and_AI/blob/main/Sheet8/Assignment8_Solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UnHoHsFop-8"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, KFold, cross_val_predict\n",
        "from sklearn.calibration import calibration_curve"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUswbzmRCHbj"
      },
      "source": [
        "def get_dataset_from_github(filename, index_col_str=None, header_str='infer'):    \n",
        "    data_file_path = \"https://raw.githubusercontent.com/FabriceBeaumont/4216_Biomedical_DS_and_AI/tree/main/Datasets\"\n",
        "    if index_col_str is None and header_str == 'infer':\n",
        "      data = pd.read_csv(data_file_path + filename)\n",
        "    elif index_col_str is None:\n",
        "        data = pd.read_csv(data_file_path + filename, header=header_str)\n",
        "    elif header_str == 'infer':\n",
        "      data = pd.read_csv(data_file_path + filename, index_col=index_col_str)\n",
        "    else:\n",
        "      data = pd.read_csv(data_file_path + filename, index_col=index_col_str, header=header_str)\n",
        "\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "Q3YXStZTEQGa",
        "outputId": "8cf94bee-6c40-4ef3-ccc3-1e1940198a32"
      },
      "source": [
        "# titanic_survival_ds = get_dataset_from_github(\"/titanic_survival_data.csv\")\n",
        "# If this does not work, load the file (temporarily) into the Colab-File system (left side) \n",
        "# from your local files. Then execute as usual:\n",
        "titanic_survival_ds = pd.read_csv(\"titanic_survival_data.csv\")\n",
        "\n",
        "titanic_survival_ds.head(4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Embarked</th>\n",
              "      <th>no_cabin</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PassengerId  Pclass  Sex   Age  ...     Fare  Embarked  no_cabin  Label\n",
              "0            1       3    0  22.0  ...   7.2500         0         2      0\n",
              "1            2       1    1  38.0  ...  71.2833         1         1      1\n",
              "2            3       3    1  26.0  ...   7.9250         0         2      1\n",
              "3            4       1    1  35.0  ...  53.1000         0         1      1\n",
              "\n",
              "[4 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx0G72bGl7xc"
      },
      "source": [
        "## Biomedical Data Science & AI\n",
        "\n",
        "## Assignment 8\n",
        "\n",
        "#### Group members:  Fabrice Beaumont, Fatemeh Salehi, Genivika Mann, Helia Salimi, Jonah"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fhYMe5ml7xo"
      },
      "source": [
        "---\n",
        "### Exercise 1 - Ensemble Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_krKhWqo-Bv7"
      },
      "source": [
        "#### 1.1. Inform yourself about **gradient boosting**, then answer the following questions in your own words:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTjkofc690tY"
      },
      "source": [
        "In-depth resource for Gradient Boosting: https://explained.ai/gradient-boosting/index.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYHqa8s-92O3"
      },
      "source": [
        "Gradient Boosting is a machine learning technique which uses Gradient Descent and Boosting. It aims at fitting an additive model by introducing **weak learners** (i.e Decision trees) such that the recently added weak learner compensates the shortcomings of existing weak learners. The shortcoming of existing weak learners are identified by gradients in the loss function. Any user specified loss function can be optimised by a gradient boosting algorithm. The objective is to minimise the loss function by adding weak learners using Gradient Descent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgUkN-nGziCe"
      },
      "source": [
        "a. What do the individual **weak learners** model? How does this relate to the\n",
        "gradient of the loss function?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKBxLpeY-Wf6"
      },
      "source": [
        "- The weak learners are trained with the objective of minimising the loss function, hence they are trained on the residuals of the model. Each new weak learner will be fitted on the **residual error** usually known as **pseudo-residual** produced by the existing sequence of learners.\n",
        "\n",
        "- The gradient boosting algorithm performs **gradient descent minimisation on some loss function** between the true and the predicted values. We perform gradient descent to bring the predicted values closer to the true value by minimising the residual. The residual is a vector which not only provides the magnitude of difference between the true and the predicted value but also the direction of better approximation (w.r.t. minimization of loss function). Hence we are chasing the (negative) gradient of the loss function via gradient descent by chasing the direction of residual. Thus we perform gradient descent on the loss function.\n",
        "\n",
        "- The gradient boosted model that trains weak learners on residual vectors optimises the mean squared error (MSE; $L_2$ loss), ...\n",
        "\n",
        "- ...while the model that trains the weak learners on the sign vector (only direction of residual without the magnitude) optimises the mean absolute error(MAE; $L_1$ loss)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rh-CgF8k-JLD"
      },
      "source": [
        "b. What is the difference between **gradient boosting** and **random forest**?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZivxSdGg-l9T"
      },
      "source": [
        "- Gradient Boosting (GB) is a forward stage-wise additive model, that builds and adds one tree at a time with the objective of minimising the loss function (computed by considering the existing sequence of trees). Random forest (RF) on the other hand builds all trees independently - using random samples of the data (to prevent overfitting).\n",
        "\n",
        "\n",
        "- GB focusses step by step on difficult examples - making it suitable for datasets with class imbalance. No such quality is present in RF. Additionally, any user specified loss function can be optimised by a gradient boosting algorithm.\n",
        "\n",
        "\n",
        "- RF combines the results of all the trees at the end after the construction of all trees. GB on the other hand, takes the predictions of the sequence of trees into consideration at each stage of the algorithm.\n",
        "\n",
        "\n",
        "- If the parameters are tuned carefully, GB can perform better than RF. However it is difficult to tune GB since there are much more parameters that need to be tuned.\n",
        "\n",
        "\n",
        "- GB is more sensitive to overfitting if the data is noisy. RF is more robust and should be considered in this case.\n",
        "\n",
        "\n",
        "- Training GB generally takes longer then RF, since the trees are constructed sequentially."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "appoDszfFI5l"
      },
      "source": [
        "#### 1.2. Which modifications make gradient boosting **robust against overfitting**?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zAnH_KU-qip"
      },
      "source": [
        "Gredient Boosting is not robust against overfitting the training data as it is a greedy algorithm. This problem can be resolved by using regualarization methods which penalize different aspects of the algorithm. The following methods can be used:\n",
        "- **Tree constraints:** The idea is that is the trees are more constrained, more trees need to be constructed. The constraints can be imposed on \n",
        "    - the number of trees (~\"keep on adding trees until no improvement is observed\"), \n",
        "    - tree depth (~\"shorter trees are preferred as deeper trees are considered more complex\"),\n",
        "    - number of nodes/leaves of tree and\n",
        "    - number of observations per split and minimum improvement to loss.\n",
        "\n",
        "- **Weighted Updates:** The prediction of each tree is weighed by a learning rate or shrinkage to slow down the learning by the algorithm. Shrinkage reduces the influence of each individual tree and leaves space for future trees to improve the model.\n",
        "\n",
        "\n",
        "- **Stochastic Gradient Boosting:** The method aims at reducing the correlation of the trees in the sequence of trees. This is achieved by using only a subsample of the training data to fit the  base learner.\n",
        "\n",
        "\n",
        "- **Penalized Gradient Boosting:** Regression trees (a variant of decision trees which contain only numeric values at leaf nodes) can be used in GB. The leaf values act as weights and can be regularised using $L_1$ or $L_2$ regularization to prevent overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyw9E17lFKxz"
      },
      "source": [
        "#### 1.3. Using the `titanic_survival_dataset.csv`, train the following models using nested cross validation while optimizing a selected number of hyperparameters in the inner loop using grid search, then compute the probabilities of your targets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdkdJx3c-ycE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "4b6986b8-4b27-4847-b8c9-415a97d87c15"
      },
      "source": [
        "# Load the dataset\n",
        "titanic_data = pd.read_csv('titanic_survival_data.csv', index_col=\"PassengerId\")\n",
        "titanic_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Embarked</th>\n",
              "      <th>no_cabin</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PassengerId</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Pclass  Sex   Age  SibSp  ...     Fare  Embarked  no_cabin  Label\n",
              "PassengerId                            ...                                    \n",
              "1                 3    0  22.0      1  ...   7.2500         0         2      0\n",
              "2                 1    1  38.0      1  ...  71.2833         1         1      1\n",
              "3                 3    1  26.0      0  ...   7.9250         0         2      1\n",
              "4                 1    1  35.0      1  ...  53.1000         0         1      1\n",
              "5                 3    0  35.0      0  ...   8.0500         0         2      0\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaF0u7bfXh8W"
      },
      "source": [
        "# Sepearate features and target (which is stored in column 'Label')\n",
        "y = titanic_data['Label'].ravel()\n",
        "X = titanic_data.drop(columns = ['Label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhfmrS-AFOra"
      },
      "source": [
        "#### 1.3.a) Random forest, optimizing the number of estimators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53pNibdz_vh7"
      },
      "source": [
        "# Initialize the RF classifier and a parameter grid for the grid search\n",
        "Random_Forest = RandomForestClassifier()\n",
        "p_grid_random_forest = {'n_estimators': [100, 150, 200, 300, 400]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2WoTW8B_xzP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c3c5ad9-e3f1-45e2-d496-ff79de3811a2"
      },
      "source": [
        "# Inner Fold - to obtain the best hyperparameters\n",
        "Random_Forest_Fit = GridSearchCV(\n",
        "    estimator = Random_Forest,\n",
        "    param_grid = p_grid_random_forest,\n",
        "    cv = KFold(shuffle = True),\n",
        "    verbose = 1\n",
        ")\n",
        "\n",
        "Random_Forest_Fit.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:   10.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=KFold(n_splits=5, random_state=None, shuffle=True),\n",
              "             error_score=nan,\n",
              "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
              "                                              class_weight=None,\n",
              "                                              criterion='gini', max_depth=None,\n",
              "                                              max_features='auto',\n",
              "                                              max_leaf_nodes=None,\n",
              "                                              max_samples=None,\n",
              "                                              min_impurity_decrease=0.0,\n",
              "                                              min_impurity_split=None,\n",
              "                                              min_samples_leaf=1,\n",
              "                                              min_samples_split=2,\n",
              "                                              min_weight_fraction_leaf=0.0,\n",
              "                                              n_estimators=100, n_jobs=None,\n",
              "                                              oob_score=False,\n",
              "                                              random_state=None, verbose=0,\n",
              "                                              warm_start=False),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'n_estimators': [100, 150, 200, 300, 400]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81oe6w3d_6QW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95cebe00-e1e2-4dc4-b4b5-3c257a70781a"
      },
      "source": [
        "# Outer Fold - to perform cross validation based on metrics and compute the probabilities of the target\n",
        "random_forest_prediction_prob = cross_val_predict(\n",
        "    estimator = Random_Forest_Fit,\n",
        "    X = X,\n",
        "    y = y,\n",
        "    cv = KFold(shuffle = True),\n",
        "    method = 'predict_proba', # To obtain prediction probabilities in result\n",
        "    verbose = 1\n",
        ")\n",
        "random_forest_prediction_prob"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    9.6s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    9.5s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    9.4s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    9.5s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    9.5s finished\n",
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   50.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.706  , 0.294  ],\n",
              "       [0.01   , 0.99   ],\n",
              "       [0.615  , 0.385  ],\n",
              "       ...,\n",
              "       [0.605  , 0.395  ],\n",
              "       [0.155  , 0.845  ],\n",
              "       [0.98375, 0.01625]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NT5fcXYbFQKJ"
      },
      "source": [
        "#### 1.3.b) Gradient boosting, optimizing boosting steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqLsNOin_-zz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90ee8009-859d-4f8a-9add-f7036fde0f95"
      },
      "source": [
        "# Initialize the GB classifier and a parameter grid for the grid search\n",
        "GB = GradientBoostingClassifier()\n",
        "p_grid_gb = {'n_estimators': [10, 50, 100, 200, 300]}\n",
        "\n",
        "# Inner Fold - to obtain the best hyperparameters\n",
        "GB_Best_Clf = GridSearchCV(\n",
        "    estimator = GB,\n",
        "    param_grid = p_grid_gb,\n",
        "    cv = KFold(shuffle = True),\n",
        "    verbose = 1\n",
        ")\n",
        "\n",
        "GB_Best_Clf.fit(X, y)\n",
        "\n",
        "# Outer Fold - to perform cross validation based on metrics and compute the probabilities of the target\n",
        "gb_prediction_prob = cross_val_predict(\n",
        "    estimator = GB_Best_Clf,\n",
        "    X = X,\n",
        "    y = y,\n",
        "    cv = KFold(shuffle = True),\n",
        "    method = 'predict_proba', # To obtain prediction probabilities in result\n",
        "    verbose = 1\n",
        ")\n",
        "gb_prediction_prob"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    3.4s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    3.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    3.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    3.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    3.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    3.1s finished\n",
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   16.0s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.78829091, 0.21170909],\n",
              "       [0.05449812, 0.94550188],\n",
              "       [0.49750538, 0.50249462],\n",
              "       ...,\n",
              "       [0.54399186, 0.45600814],\n",
              "       [0.10972886, 0.89027114],\n",
              "       [0.9760037 , 0.0239963 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFoW86HYFRQr"
      },
      "source": [
        "#### 1.3.c) Lasso penalized logistic regression, optimizing $L_1$ regularization strength"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2VF4tZKAD_X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dce56d9a-4a44-4ba5-f170-f8b12ca56302"
      },
      "source": [
        "# Initialize the Logistic Regression classifier and a parameter grid for the grid search\n",
        "LR = LogisticRegression(penalty = 'l1', solver = 'liblinear')\n",
        "p_grid_lr = {'C': [1000, 100, 10, 1, 0.1]}\n",
        "\n",
        "# Inner Fold - to obtain the best hyperparameters\n",
        "LR_Best_Clf = GridSearchCV(\n",
        "    estimator = LR,\n",
        "    param_grid = p_grid_lr,\n",
        "    cv = KFold(shuffle = True),\n",
        "    verbose = 1\n",
        ")\n",
        "\n",
        "LR_Best_Clf.fit(X, y)\n",
        "\n",
        "# Outer Fold - to perform cross validation based on metrics and compute the probabilities of the target\n",
        "lr_prediction_prob = cross_val_predict(\n",
        "    estimator = LR_Best_Clf,\n",
        "    X = X,\n",
        "    y = y,\n",
        "    cv = KFold(shuffle = True),\n",
        "    method = 'predict_proba', # To obtain prediction probabilities in result\n",
        "    verbose = 1\n",
        ")\n",
        "lr_prediction_prob"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.8s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.92331682, 0.07668318],\n",
              "       [0.07173476, 0.92826524],\n",
              "       [0.41955762, 0.58044238],\n",
              "       ...,\n",
              "       [0.44867269, 0.55132731],\n",
              "       [0.32413817, 0.67586183],\n",
              "       [0.87865206, 0.12134794]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8u4nd5FAH8d"
      },
      "source": [
        "# Converting computed prediction probabilities to dataframes\n",
        "lr_prob_result = pd.DataFrame(lr_prediction_prob, columns = ['0', '1'])\n",
        "gb_prob_result = pd.DataFrame(gb_prediction_prob, columns = ['0', '1'])\n",
        "rf_prob_result = pd.DataFrame(random_forest_prediction_prob, columns = ['0', '1'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTrbd1slCDb7"
      },
      "source": [
        "(Using a large parameter grid results in an extended computation time. We advise using a maximum of *five* values per hyperparameter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NExXYSYPFL78"
      },
      "source": [
        "#### 1.4. Inform yourself about **calibration curves** (reliability diagrams). Use the predicted probabilities of each model from 3) to plot a calibration curve, then explain your results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hl0oasQ2AMD2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "e88963a2-249e-4f7a-acb5-c143ab39e74e"
      },
      "source": [
        "# Compute fraction of positives and the mean predicted value for plotting\n",
        "fraction_of_positives_rf, mean_predicted_value_rf = calibration_curve(y, rf_prob_result['1'], n_bins = 10)\n",
        "fraction_of_positives_lr, mean_predicted_value_lr = calibration_curve(y, lr_prob_result['1'], n_bins = 10)\n",
        "fraction_of_positives_gb, mean_predicted_value_gb = calibration_curve(y, gb_prob_result['1'], n_bins = 10)\n",
        "\n",
        "plt.plot(mean_predicted_value_gb, fraction_of_positives_gb, marker = '.', label = 'Gradient Boosting')\n",
        "plt.plot(mean_predicted_value_rf, fraction_of_positives_rf, marker = '.', label = 'Random Forest')\n",
        "plt.plot(mean_predicted_value_lr, fraction_of_positives_lr, marker = '.', label = 'Logistic Regression')\n",
        "plt.plot([0,1], [0,1], linestyle = '--',label = 'Perfectly Calibrated')\n",
        "\n",
        "plt.xlabel('Mean Predicted Value')\n",
        "plt.ylabel('Fraction of positives')\n",
        "plt.title('Reliability Curves')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28Eku42XAUlZ"
      },
      "source": [
        "Calibration diagrams show how the predicted probabilities of the model are well calibrated  with respect to the true distribution of the class in data. The closer the plot is to the main diagonal, the better calibrated are the probabilities of the model. If the curve is below the main diagonal, the model has *overforcast* (the probabilities are too large). On the other hand, if the curve is above the main diagonal, the model has *underforcast* (the probabilities are too small).\n",
        "\n",
        "\n",
        "**Inference from Calibration Curves:** \n",
        "The GB model and the Logistic Regression are the most calibrated. In the plot, their predicted probabilities almost overlap with the main diagonal for some mean predicted values. \n",
        "RF overforcasts and underforcasts the most between the mean predicted value of $0.6$ and $0.8$. None of the models show very poor performance. This can be because we performed nested cross validation before reporting the results. All the plots are close to the perfect calibration line."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v3MrpiqElLz"
      },
      "source": [
        "---\n",
        "### Exercise 2 - Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsbQAaMbY7q1"
      },
      "source": [
        "#### 2.1. Suppose there is a Multi-Layer Perceptron (MLP) composed of one input layer with *eight* neurons, followed by one hidden layer with *30* artificial neurons, and one output layer with *three* artificial neurons. All artificial neurons use the ReLU activation function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjUHF7RUE3Cp"
      },
      "source": [
        "##### 2.1.a) Deduce the shape of input matrix $X$, hidden layer’s weight vector $W_h$, bias vector $b_h$ and the shape of the network’s output matrix $Y$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIvBLUd4Tq8Z"
      },
      "source": [
        "If we suppose each $x_i$ has n features then: $X.shape = (n, 8)$ and $W_h.shape = (30,8)$ <br>\n",
        "So the shape of the term $XW_h^T$ will be $(n, 30)$ and it should be the same for $b_h^T \\Longrightarrow b_h.shape = (30, n)$<br>\n",
        "$H$ have the shape $(n, 30)$ and $W_o.shape = (3, 30)$ so the shape of the term $HW_o^T$ will be $(n, 3)$ and it should be the same for $b_o^T \\Longrightarrow b_o.shape = (3, n) \\Longrightarrow Y.shape = (n,3)$<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oY1tnymZFBbW"
      },
      "source": [
        "##### 2.1.b) Write the equation that computes the network’s output matrix $Y$ as a function of $X$, $W_h$, $b_h$, $W_o$ and $b_o$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIxEwTAhbDit"
      },
      "source": [
        "activation function: $ReLU(X) =$ [ $max(0,x_i)$ for $x_i$ in $X$ ] <br>\n",
        "$H = XW_h^T + b_h^T$ <br> $out_H = ReLU(H)$ <br>\n",
        "$Y = out_HW_o^T + b_o^T$ <br> $out_Y = ReLU(Y)$ <br>\n",
        "$\\Longrightarrow out_Y = ReLU((ReLU(XW_h^T + b_h^T)) W_o^T + b_o^T)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6uiZwAlCnlr"
      },
      "source": [
        "#### 2.2. What are principle and unavoidable limitations of the backpropagation (BP)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOesRBz6Jz_A"
      },
      "source": [
        "- **Local Minima Problem:** If local minima are present along with global minima in the synaptic weight space, BP algorithm will get trapped in the local minima. Since in this situation, every small change in synaptic weights will increase the cost function.\n",
        "\n",
        "\n",
        "- **Slow Convergence:** The BP algorithm uses an instantaneous estimate for the gradient of the error-surface in weight space. Due to this stochastic nature of the algorithm, it tends to converge slowly. If the error surface along a weight dimension is flat, the gradient will be small resulting in small updates to the weights. Hence the algorithm will have to undergo a large no. of iterations. In the opposite scenario, if the error surface is curved, the magnitude of gradient will be large causing larger weigth updates and the algorithm might overshoot the minimum of the error surface.\n",
        "\n",
        "\n",
        "- **Scaling:** It does not scale well to large problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UswnbGnvFDY5"
      },
      "source": [
        "#### 2.3. The shown figure is a *three* layer neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_o6m1LbBWyv"
      },
      "source": [
        "##### 2.3.a) Compute $h_1$, $h_2$, $o_1$, and the total error using ReLU units. \n",
        "\n",
        "*Note*: $b_1$, $b_2$ and $b_3$ represent the biases added to their respective units."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEH0wipMxzc5"
      },
      "source": [
        "$h_1 = i_1 \\times w_1 + i_2 \\times w_2 + b_1 = 0.5 \\times 0.15 + 0.8 \\times 0.2 + 0.4  = 0.635$<br>\n",
        "$out_{h_1} = ReLU(h_1) = max(0.635, 0) = 0.635$ <br>\n",
        "$h_2 = i_1 \\times w_3 + i_2 \\times w_4 + b_2 = 0.5 \\times 0.25 + 0.8 \\times 0.3 + 0.3 = 0.665$<br>\n",
        "$out_{h_2} = ReLU(h_2) = max(0.665, 0) = 0.665$ <br>\n",
        "$o_1 = out_{h_1} \\times w_5 + out_{h_2} \\times w_6 + b_3 = 0.635 \\times 0.4 + 0.665 \\times 0.55 + 0.6 = 1.21975$\n",
        "$out_{o_1} = ReLU(o_1) = max(1.21975, 0) = 1.21975$ <br>\n",
        "$Error_{total} = Error_1 = \\frac{1}{2}(1 - 1.21975)^2 = 0.024145031249999976$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BU705EzJBeP4"
      },
      "source": [
        "##### 2.3.b) Calculate the updates of the network weights $w_1, \\dots , w_6$ and bias terms $b_1$, $b_2$, $b_3$ using backpropagation. Assume a learning rate of $1$ for the sake of simplicity. \n",
        "\n",
        "*Note*: Remember that a bias term is equivalent to a weighted constant input 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVZHOG2JYQlU"
      },
      "source": [
        "Learning rate: $\\gamma = 1$<br><br>\n",
        "$Error_{total} = \\frac{1}{2}(T_1 - out_{o_1})^2$ <br><br>\n",
        "$ReLU(x) = \\begin{cases} x & \\text{if } x \\geq 0\\\\ 0 & \\text{if }x \\lt 0\\end{cases}$ <br>\n",
        "$\\lim_{h \\to 0^+} \\dfrac{f(h) - f(0)}{h} = \\lim_{h\\to 0^+} \\dfrac{h}{h} = 1$<br>\n",
        "$\\lim_{h \\to 0^-} \\dfrac{f(h) - f(0)}{h} = 0$<br>\n",
        "$ReLU^{\\prime}(x) = \\begin{cases} 1 & \\text{if } x \\geq 0\\\\ 0 & \\text{if }x \\lt 0\\end{cases}$<br><br><br>\n",
        "\n",
        "**Error at** $\\pmb{w_6}$: $\\frac{\\partial E_{total}}{\\partial w_6} = \n",
        "\\frac{\\partial E_{total}}{\\partial out_{o_1}} \\times \n",
        "\\frac{\\partial out_{o_1}}{\\partial o_1} \\times \n",
        "\\frac{\\partial o_1}{\\partial w_6} $\n",
        "\n",
        "\n",
        "$\\frac{\\partial E_{total}}{\\partial out_{o_1}} = 2 \\times \\frac{1}{2}(T_1 - out_{o_1}) (-1) = -T_1 + out_{o_1} = -1+1.21975 = 0.21975$\n",
        "\n",
        "\n",
        "\n",
        "$\\frac{\\partial out_{o_1}}{\\partial o_1} = 1$\n",
        "\n",
        "$\\frac{\\partial o_1}{\\partial w_6} = 1 \\times out_{h_2} = 0.665$\n",
        "\n",
        "$\\Longrightarrow \\frac{\\partial E_{total}}{\\partial w_6} = 0.21975 \\times 1 \\times 0.665 = 0.14613375$<br>\n",
        "**updating** $\\pmb{w_6}: w_6 = w_6-\\gamma \\times \\frac{\\partial E_{total}}{\\partial w_6} = 0.55 - 1 \\times 0.14613375 = 0.40386625$<br><br>\n",
        "\n",
        "**Error at** $\\pmb{w_5}$: $\\frac{\\partial E_{total}}{\\partial w_5} = \n",
        "\\frac{\\partial E_{total}}{\\partial out_{o_1}} \\times \n",
        "\\frac{\\partial out_{o_1}}{\\partial o_1} \\times \n",
        "\\frac{\\partial o_1}{\\partial w_5} $<br>\n",
        "\n",
        "$\\frac{\\partial E_{total}}{\\partial out_{o_1}} = 0.21975$<br>\n",
        "$\\frac{\\partial out_{o_1}}{\\partial o_1} = 1$<br>\n",
        "$\\frac{\\partial o_1}{\\partial w_5} = 1 \\times out_{h_1} = 0.635$\n",
        "\n",
        "$\\Longrightarrow \\frac{\\partial E_{total}}{\\partial w_5} = 0.21975 \\times 1 \\times 0.635 = 0.13954125$<br>\n",
        "**updating** $\\pmb{w_5}: w_5 = w_5-\\gamma \\times \\frac{\\partial E_{total}}{\\partial w_5} = 0.4 - 1 \\times 0.13954125 = 0.26045875$<br><br>\n",
        "\n",
        "**Error at** $\\pmb{w_4}$: $\\frac{\\partial E_{total}}{\\partial w_4} = \n",
        "\\frac{\\partial E_{total}}{\\partial out_{o_1}} \\times \\frac{\\partial out_{o_1}}{\\partial o_1} \\times \n",
        "\\frac{\\partial o_1}{\\partial out_{h_2}} \\times \n",
        "\\frac{\\partial out_{h_2}}{\\partial h_2} \\times \n",
        "\\frac{\\partial h_2}{\\partial w_4} $\n",
        "\n",
        "$\\frac{\\partial E_{total}}{\\partial out_{o_1}} = 0.21975$<br>\n",
        "$\\frac{\\partial out_{o_1}}{\\partial o_1} = 1$<br>\n",
        "$\\frac{\\partial o_1}{\\partial out_{h_2}} = 1 \\times w_6 = 0.55$<br>\n",
        "$\\frac{\\partial out_{h_2}}{\\partial h_2} = 1$<br>\n",
        "$\\frac{\\partial h_2}{\\partial w_4} = 1 \\times i_2 = 0.8$\n",
        "\n",
        "$\\Longrightarrow \\frac{\\partial E_{total}}{\\partial w_4} = 0.21975 \\times 1 \\times 0.55 \\times 1 \\times 0.8 = 0.09669$<br>\n",
        "**updating** $\\pmb{w_4}: w_4 = w_4-\\gamma \\times \\frac{\\partial E_{total}}{\\partial w_4} = 0.3 - 1 \\times 0.09669 = 0.20331$<br><br>\n",
        "\n",
        "**Error at** $\\pmb{w_3}$: $\\frac{\\partial E_{total}}{\\partial w_3} = \n",
        "\\frac{\\partial E_{total}}{\\partial out_{o_1}} \\times \\frac{\\partial out_{o_1}}{\\partial o_1} \\times \n",
        "\\frac{\\partial o_1}{\\partial out_{h_2}} \\times \n",
        "\\frac{\\partial out_{h_2}}{\\partial h_2} \\times \n",
        "\\frac{\\partial h_2}{\\partial w_3} $\n",
        "\n",
        "$\\frac{\\partial E_{total}}{\\partial out_{o_1}} = 0.21975$<br>\n",
        "$\\frac{\\partial out_{o_1}}{\\partial o_1} = 1$<br>\n",
        "$\\frac{\\partial o_1}{\\partial out_{h_2}} = 0.55$<br>\n",
        "$\\frac{\\partial out_{h_2}}{\\partial h_2} = 1$<br>\n",
        "$\\frac{\\partial h_2}{\\partial w_3} = 1 \\times i_1 = 0.5$\n",
        "\n",
        "$\\Longrightarrow \\frac{\\partial E_{total}}{\\partial w_4} = 0.21975 \\times 1 \\times 0.55 \\times 1 \\times 0.5 = 0.06043125$<br>\n",
        "**updating** $\\pmb{w_3}: w_3 = w_3-\\gamma \\times \\frac{\\partial E_{total}}{\\partial w_3} = 0.25 - 1 \\times 0.06043125 = 0.18956875$<br><br>\n",
        "\n",
        "**Error at** $\\pmb{w_2}$: $\\frac{\\partial E_{total}}{\\partial w_2} = \n",
        "\\frac{\\partial E_{total}}{\\partial out_{o_1}} \\times \\frac{\\partial out_{o_1}}{\\partial o_1} \\times \n",
        "\\frac{\\partial o_1}{\\partial out_{h_1}} \\times \n",
        "\\frac{\\partial out_{h_1}}{\\partial h_1} \\times \n",
        "\\frac{\\partial h_1}{\\partial w_2} $\n",
        "\n",
        "$\\frac{\\partial E_{total}}{\\partial out_{o_1}} = 0.21975$<br>\n",
        "$\\frac{\\partial out_{o_1}}{\\partial o_1} = 1$<br>\n",
        "$\\frac{\\partial o_1}{\\partial out_{h_1}} = 1 \\times w_5 = 0.4$<br>\n",
        "$\\frac{\\partial out_{h_1}}{\\partial h_1} = 1$<br>\n",
        "$\\frac{\\partial h_1}{\\partial w_2} = 1 \\times i_2 = 0.8$\n",
        "\n",
        "$\\Longrightarrow \\frac{\\partial E_{total}}{\\partial w_2} = 0.21975 \\times 1 \\times 0.4 \\times 1 \\times 0.8 = 0.07032$<br>\n",
        "**updating** $\\pmb{w_2}: w_2 = w_2-\\gamma \\times \\frac{\\partial E_{total}}{\\partial w_2} = 0.2 - 1 \\times 0.07032 = 0.12968$<br><br>\n",
        "\n",
        "**Error at** $\\pmb{w_1}$: $\\frac{\\partial E_{total}}{\\partial w_1} = \n",
        "\\frac{\\partial E_{total}}{\\partial out_{o_1}} \\times \\frac{\\partial out_{o_1}}{\\partial o_1} \\times \n",
        "\\frac{\\partial o_1}{\\partial out_{h_1}} \\times \n",
        "\\frac{\\partial out_{h_1}}{\\partial h_1} \\times \n",
        "\\frac{\\partial h_1}{\\partial w_1} $\n",
        "\n",
        "$\\frac{\\partial E_{total}}{\\partial out_{o_1}} = 0.21975$<br>\n",
        "$\\frac{\\partial out_{o_1}}{\\partial o_1} = 1$<br>\n",
        "$\\frac{\\partial o_1}{\\partial out_{h_1}} = 1 \\times w_5 = 0.4$<br>\n",
        "$\\frac{\\partial out_{h_1}}{\\partial h_1} = 1$<br>\n",
        "$\\frac{\\partial h_1}{\\partial w_1} = 1 \\times i_1 = 0.5$\n",
        "\n",
        "$\\Longrightarrow \\frac{\\partial E_{total}}{\\partial w_1} = 0.21975 \\times 1 \\times 0.4 \\times 1 \\times 0.5 = 0.04395$<br>\n",
        "**updating** $\\pmb{w_1}: w_1 = w_1-\\gamma \\times \\frac{\\partial E_{total}}{\\partial w_1} = 0.15 - 1 \\times 0.04395 = 0.10605$<br><br>\n",
        "\n",
        "**Error at** $\\pmb{b_3}$: $\\frac{\\partial E_{total}}{\\partial b_3} = \n",
        "\\frac{\\partial E_{total}}{\\partial out_{o_1}} \\times \n",
        "\\frac{\\partial out_{o_1}}{\\partial o_1} \\times \n",
        "\\frac{\\partial o_1}{\\partial b_3} $<br>\n",
        "\n",
        "$\\frac{\\partial E_{total}}{\\partial out_{o_1}} = 0.21975$<br>\n",
        "$\\frac{\\partial out_{o_1}}{\\partial o_1} = 1$<br>\n",
        "$\\frac{\\partial o_1}{\\partial b_3} = 1$\n",
        "\n",
        "$\\Longrightarrow \\frac{\\partial E_{total}}{\\partial b_3} = 0.21975 \\times 1 \\times 1 = 0.21975$<br>\n",
        "**updating** $\\pmb{b_3}: b_3 = b_3-\\gamma \\times \\frac{\\partial E_{total}}{\\partial b_3} = 0.6 - 1 \\times 0.21975 = 0.38025$<br><br>\n",
        "\n",
        "**Error at** $\\pmb{b_2}$: $\\frac{\\partial E_{total}}{\\partial b_2} = \n",
        "\\frac{\\partial E_{total}}{\\partial out_{o_1}} \\times \\frac{\\partial out_{o_1}}{\\partial o_1} \\times \n",
        "\\frac{\\partial o_1}{\\partial out_{h_2}} \\times \n",
        "\\frac{\\partial out_{h_2}}{\\partial h_2} \\times \n",
        "\\frac{\\partial h_2}{\\partial b_2} $\n",
        "\n",
        "$\\frac{\\partial E_{total}}{\\partial out_{o_1}} = 0.21975$<br>\n",
        "$\\frac{\\partial out_{o_1}}{\\partial o_1} = 1$<br>\n",
        "$\\frac{\\partial o_1}{\\partial out_{h_2}} = 0.55$<br>\n",
        "$\\frac{\\partial out_{h_2}}{\\partial h_2} = 1$<br>\n",
        "$\\frac{\\partial h_2}{\\partial b_2} = 1$\n",
        "\n",
        "$\\Longrightarrow \\frac{\\partial E_{total}}{\\partial b_2} = 0.21975 \\times 1 \\times 0.55 \\times 1 \\times 1 = 0.1208625$<br>\n",
        "**updating** $\\pmb{b_2}: b_2 = b_2-\\gamma \\times \\frac{\\partial E_{total}}{\\partial b_2} = 0.3 - 1 \\times 0.1208625 = 0.1791375$<br><br>\n",
        "\n",
        "**Error at** $\\pmb{b_1}$: $\\frac{\\partial E_{total}}{\\partial b_1} = \n",
        "\\frac{\\partial E_{total}}{\\partial out_{o_1}} \\times \\frac{\\partial out_{o_1}}{\\partial o_1} \\times \n",
        "\\frac{\\partial o_1}{\\partial out_{h_1}} \\times \n",
        "\\frac{\\partial out_{h_1}}{\\partial h_1} \\times \n",
        "\\frac{\\partial h_1}{\\partial b_1} $\n",
        "\n",
        "$\\frac{\\partial E_{total}}{\\partial out_{o_1}} = 0.21975$<br>\n",
        "$\\frac{\\partial out_{o_1}}{\\partial o_1} = 1$<br>\n",
        "$\\frac{\\partial o_1}{\\partial out_{h_1}} = 1 \\times w_5 = 0.4$<br>\n",
        "$\\frac{\\partial out_{h_1}}{\\partial h_1} = 1$<br>\n",
        "$\\frac{\\partial h_1}{\\partial w_1} = 1$\n",
        "\n",
        "$\\Longrightarrow \\frac{\\partial E_{total}}{\\partial b_1} = 0.21975 \\times 1 \\times 0.4 \\times 1 \\times 1 = 0.0879$<br>\n",
        "**updating** $\\pmb{b_1}: b_1 = b_1-\\gamma \\times \\frac{\\partial E_{total}}{\\partial b_1} = 0.4 - 1 \\times 0.0879 = 0.3121$<br><br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBabTHc7_3Qp",
        "outputId": "52c0849d-1a1a-47b9-87dc-2e76eebe232b"
      },
      "source": [
        "!wget[](https://raw.githubusercontent.com/FabriceBeaumont/4216_Biomedical_DS_and_AI/main/Sheet8/sheet8_NN_sketch.png)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: -c: line 0: syntax error near unexpected token `https://raw.githubusercontent.com/FabriceBeaumont/4216_Biomedical_DS_and_AI/main/Sheet8/sheet8_NN_sketch.png'\n",
            "/bin/bash: -c: line 0: `wget[](https://raw.githubusercontent.com/FabriceBeaumont/4216_Biomedical_DS_and_AI/main/Sheet8/sheet8_NN_sketch.png)'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtFd0JZPDVK4"
      },
      "source": [
        "With:\n",
        "- $i_1 = 0.5$, $i_2 = 0.8$\n",
        "- $w_1 = 0.15$, $w_2 = 0.2$, $w_3 = 0.25$, $w_4 = 0.3$, $w_5 = 0.4$, $w_6 = 0.55$\n",
        "- $b_1 = 0.4$, $w_3 = 0.6$\n",
        "\n",
        "And the activation function for $h_1$ and $h_2$ is the ReLU. The expected outputis $1$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aulNH2BmC5KU"
      },
      "source": [
        "---\n",
        "### Exercise 3- NN Programming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXYQj-xFD0-5"
      },
      "source": [
        "#### 3.1. Familiarize yourself with **tensorflow** and train a neural network with *two* hidden layers ($10$ and $8$ units respectively) and predict the label feature using the `titanic_survival_dataset.csv` dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmrdJKSXD5sR"
      },
      "source": [
        "#### 3.2. Evaluate the performance of the neural network for the same dataset in a nested cross validation by optimizing the number of units in the second hidden layer in the inner cross validation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqZp3FIBD7P_"
      },
      "source": [
        "#### 3.3. How does the neural network perform in comparison to the models in the calibration curve from the previous task and plot the results alongside the other models in the calibration plot?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yx8o12VEMOg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}