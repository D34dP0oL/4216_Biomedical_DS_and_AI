{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Assignment_10_Solutions.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FabriceBeaumont/4216_Biomedical_DS_and_AI/blob/main/Sheet10/Assignment_10_Solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9UZQvv0GGYB"
      },
      "source": [
        "## Biomedical Data Science & AI\n",
        "\n",
        "## Assignment 10\n",
        "\n",
        "#### Group members:  Fabrice Beaumont, Fatemeh Salehi, Genivika Mann, Helia Salimi, Jonah"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pP7FPcuSGGYE"
      },
      "source": [
        "**Exercise 1 - Variational Autoencoders (VAEs) (12 points)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZHUAvoBGGYG"
      },
      "source": [
        "**1. Explain how far a VAE’s lower dimension representation differs from that learned\n",
        "by a traditional autoencoder. How is that achieved? How far does the training\n",
        "objective differ? (2 points)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWodsOwzGGYJ"
      },
      "source": [
        "- Traditional autoencoders use the datapoints directly for encoding and minimising the reconstruction error without enforcing any organisation in the latent space(lower dimensional representation). VAE's on the other hand, assume that the input data belongs to a probability distribution over the latent space(lower dimensional representation) and attempts to find the parameters of that distribution. It maps each data point to a Gaussian distribution. This produces a smooth manifold embedding in latent space.\n",
        "\n",
        "\n",
        "- Since traditional autoencoders do not learn a probability distribution hence the mapping generated by traditional autoencoders could be discontinuous (indicated by presence of gaps in the latent space). VAE's have smooth interpolation between different groups.\n",
        "\n",
        "\n",
        "- Autoencoders cannot be used to generate new content. VAE's can be used as a generative system as they can produce new data which is related to original inputs. VAE's can be considered as autoencoders whose training process is regularised to avoid overfitting and latent space has the properties to enable generative process.\n",
        "\n",
        "\n",
        "- In traditional autoencoders, each hidden layer represents a non-linear mapping of previous layer.\n",
        "$$\n",
        "z = s(Wx+b)\n",
        "$$\n",
        "The decoder is responsible for reconstructing the input using the output of the encoder.\n",
        "$$\n",
        "y = s(W'z+b')\n",
        "$$\n",
        "The objective is to minimise the reconstruction error which is done by gradient descent over the parameters of the encoder and decoder neural networks.\n",
        "\n",
        "$$\n",
        "\\text{arg min}_{W, W', b, b'}\\ l(x_i, y_i)\n",
        "$$\n",
        "Possible loss functions can be squared error or cross entropy for binary data.\n",
        "\n",
        "\n",
        "- The training objective of VAE's consists of a reconstruction error term and regularisation term. It is given by:\n",
        "\n",
        "$$\n",
        "\\tilde{L}(\\theta, \\phi, x^{(i)}) = \\frac{1}{2} \\sum_{j = 1}^{J}\\left(1+log((\\sigma_j^{(i)})^2))- (\\mu_j^{(i)}))^2 - (\\sigma_j^{(i)}))^2 \\right) + \\frac{1}{L} \\sum_{l = 1}^{L}\\left(log p_\\theta(x^{(i)}|z^{(i,l)})\\right)\n",
        "$$\n",
        "\n",
        "where J = mini-batch size and L = no. of samples\n",
        "\n",
        "- Traditional autoencoders do not balance regularity (structures of latent space are intrepretable and exploitable) and reconstruction error(important information of the data must not be lost after dimensionality reduction). The VAE objective function is able to achieve this by balancing reconstruction accuracy and deviation from prior distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeaJ0CDqGGYM"
      },
      "source": [
        "**2. Inform yourself about the applications of autoencoders in the biomedical field.\n",
        "Explore the literature, then mention one application and explain how it works. (2\n",
        "points)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB_q_w4aGGYO"
      },
      "source": [
        "- In the paper titled ***'Automatic Sleep Stage Scoring Using Time-Frequency Analysis and Stacked Sparse Autoencoders'*** by Tsinalis et al $^{[1]}$, a methodolgy has been developed for automatic sleep stage scoring. An openly available dataset containing EEG data of 20 healthy young adults was used in this literature. In order to ensure that the results of this study were suitable for longitudinal monitoring using wearable EEG in real world setting, only a single channel of EEG from the dataset was used for construction of the model. Sleep was classified into 5 stages namely - N1, N2, N3, R and W.\n",
        "\n",
        "\n",
        "- Time frequency analysis based feature extraction is fine tuned to capture sleep stage specific signal features. Sleep stages are then classified using ensemble learning with an ensemble of stacked sparse autoencoders. For each model, class balanced random sampling across sleep stages was performed to deal with any class imbalance in the dataset.\n",
        "\n",
        "\n",
        "- The hyperparameter values were fixed to $\\lambda = 1 * 10^{-15}$, $\\beta = 2.0$, $\\rho=0.2$ and n = 20 (units of hidden layer), r = 60 (no. of optimisation iterations). The features were transformed so that distribution is approximately centered around mean hence sigmoid activation function was used for autoencoders. The final model consisted of an ensemble of 20 SSAEs(Stacked Sparse Autoencoders) having same hyperparameters. The results were evaluated using 20 fold cross validation.\n",
        "\n",
        "\n",
        "- The method achieved an overall accuracy in range 75-80%, high mean F1-score in range 82-86% and mean accuracy across individual sleep stages in range 84-88% over all subjects. This is a relevant application of Autoencoders as detection of sleep/circadian distruption can be vital in recognising early stages of neurodegenerative diseases such as Alzheimer's disease, Parkinson's disease, etc and sleep stabilisation can improve patient's quality of life.\n",
        "\n",
        "**References:**\n",
        "- Tsinalis, O., Matthews, P.M. & Guo, Y. Automatic Sleep Stage Scoring Using Time-Frequency Analysis and Stacked Sparse Autoencoders. Ann Biomed Eng 44, 1587–1597 (2016)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ9OIIfpGGYR"
      },
      "source": [
        "**3. Inform yourself about VAE variants, then explain the modifications and uses of\n",
        "the following variants:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54cGJeKqGGYT"
      },
      "source": [
        "**a. Beta-VAE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgh5yMcpGGYV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJ00tfoFGGYX"
      },
      "source": [
        "**b. Vector Quantised-VAE (VQ-VAE)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljdMFEU4GGYY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}