{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Assignment_10_Solutions.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FabriceBeaumont/4216_Biomedical_DS_and_AI/blob/main/Sheet10/Assignment_10_Solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9UZQvv0GGYB"
      },
      "source": [
        "## Biomedical Data Science & AI\n",
        "\n",
        "## Assignment 10\n",
        "\n",
        "#### Group members:  Fabrice Beaumont, Fatemeh Salehi, Genivika Mann, Helia Salimi, Jonah"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pP7FPcuSGGYE"
      },
      "source": [
        "**Exercise 1 - Variational Autoencoders (VAEs) (12 points)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZHUAvoBGGYG"
      },
      "source": [
        "**1. Explain how far a VAE’s lower dimension representation differs from that learned\n",
        "by a traditional autoencoder. How is that achieved? How far does the training\n",
        "objective differ? (2 points)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWodsOwzGGYJ"
      },
      "source": [
        "- Traditional autoencoders use the datapoints directly for encoding and minimising the reconstruction error without enforcing any organisation in the latent space(lower dimensional representation). VAE's on the other hand, assume that the input data belongs to a probability distribution over the latent space(lower dimensional representation) and attempts to find the parameters of that distribution. It maps each data point to a Gaussian distribution. This produces a smooth manifold embedding in latent space.\n",
        "\n",
        "\n",
        "- Since traditional autoencoders do not learn a probability distribution hence the mapping generated by traditional autoencoders could be discontinuous (indicated by presence of gaps in the latent space). VAE's have smooth interpolation between different groups.\n",
        "\n",
        "\n",
        "- Autoencoders cannot be used to generate new content. VAE's can be used as a generative system as they can produce new data which is related to original inputs. VAE's can be considered as autoencoders whose training process is regularised to avoid overfitting and latent space has the properties to enable generative process.\n",
        "\n",
        "\n",
        "- In traditional autoencoders, each hidden layer represents a non-linear mapping of previous layer.\n",
        "$$\n",
        "z = s(Wx+b)\n",
        "$$\n",
        "The decoder is responsible for reconstructing the input using the output of the encoder.\n",
        "$$\n",
        "y = s(W'z+b')\n",
        "$$\n",
        "The objective is to minimise the reconstruction error which is done by gradient descent over the parameters of the encoder and decoder neural networks.\n",
        "\n",
        "$$\n",
        "\\text{arg min}_{W, W', b, b'}\\ l(x_i, y_i)\n",
        "$$\n",
        "Possible loss functions can be squared error or cross entropy for binary data.\n",
        "\n",
        "\n",
        "- The training objective of VAE's consists of a reconstruction error term and regularisation term. It is given by:\n",
        "\n",
        "$$\n",
        "\\tilde{L}(\\theta, \\phi, x^{(i)}) = \\frac{1}{2} \\sum_{j = 1}^{J}\\left(1+log((\\sigma_j^{(i)})^2))- (\\mu_j^{(i)}))^2 - (\\sigma_j^{(i)}))^2 \\right) + \\frac{1}{L} \\sum_{l = 1}^{L}\\left(log p_\\theta(x^{(i)}|z^{(i,l)})\\right)\n",
        "$$\n",
        "\n",
        "where J = mini-batch size and L = no. of samples\n",
        "\n",
        "- Traditional autoencoders do not balance regularity (structures of latent space are intrepretable and exploitable) and reconstruction error(important information of the data must not be lost after dimensionality reduction). The VAE objective function is able to achieve this by balancing reconstruction accuracy and deviation from prior distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeaJ0CDqGGYM"
      },
      "source": [
        "**2. Inform yourself about the applications of autoencoders in the biomedical field.\n",
        "Explore the literature, then mention one application and explain how it works. (2\n",
        "points)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB_q_w4aGGYO"
      },
      "source": [
        "- In the paper titled ***'Automatic Sleep Stage Scoring Using Time-Frequency Analysis and Stacked Sparse Autoencoders'*** by Tsinalis et al $^{[1]}$, a methodolgy has been developed for automatic sleep stage scoring. An openly available dataset containing EEG data of 20 healthy young adults was used in this literature. In order to ensure that the results of this study were suitable for longitudinal monitoring using wearable EEG in real world setting, only a single channel of EEG from the dataset was used for construction of the model. Sleep was classified into 5 stages namely - N1, N2, N3, R and W.\n",
        "\n",
        "\n",
        "- Time frequency analysis based feature extraction is fine tuned to capture sleep stage specific signal features. Sleep stages are then classified using ensemble learning with an ensemble of stacked sparse autoencoders. For each model, class balanced random sampling across sleep stages was performed to deal with any class imbalance in the dataset.\n",
        "\n",
        "\n",
        "- The hyperparameter values were fixed to $\\lambda = 1 * 10^{-15}$, $\\beta = 2.0$, $\\rho=0.2$ and n = 20 (units of hidden layer), r = 60 (no. of optimisation iterations). The features were transformed so that distribution is approximately centered around mean hence sigmoid activation function was used for autoencoders. The final model consisted of an ensemble of 20 SSAEs(Stacked Sparse Autoencoders) having same hyperparameters. The results were evaluated using 20 fold cross validation.\n",
        "\n",
        "\n",
        "- The method achieved an overall accuracy in range 75-80%, high mean F1-score in range 82-86% and mean accuracy across individual sleep stages in range 84-88% over all subjects. This is a relevant application of Autoencoders as detection of sleep/circadian distruption can be vital in recognising early stages of neurodegenerative diseases such as Alzheimer's disease, Parkinson's disease, etc and sleep stabilisation can improve patient's quality of life.\n",
        "\n",
        "**References:**\n",
        "- Tsinalis, O., Matthews, P.M. & Guo, Y. Automatic Sleep Stage Scoring Using Time-Frequency Analysis and Stacked Sparse Autoencoders. Ann Biomed Eng 44, 1587–1597 (2016)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ9OIIfpGGYR"
      },
      "source": [
        "**3. Inform yourself about VAE variants, then explain the modifications and uses of\n",
        "the following variants:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54cGJeKqGGYT"
      },
      "source": [
        "**a. Beta-VAE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgh5yMcpGGYV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJ00tfoFGGYX"
      },
      "source": [
        "**b. Vector Quantised-VAE (VQ-VAE)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljdMFEU4GGYY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUC2udauJAZw"
      },
      "source": [
        "**Exercise 2 - Generative Adversarial Networks (GANs)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vu_2noeYJSiI"
      },
      "source": [
        "**1. What are the specific properties of GANs in comparison to VAEs?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dHxq15fJXQM"
      },
      "source": [
        "The images of the VAE are blurry and no realistic objects can be recognized. The GAN generates images with sharper edges.<br>\n",
        "The GAN produces much sharper images than the VAE.<br>\n",
        "The faces produced by the VAE own a more natural appearance.<br>\n",
        "The main difference between VAEs and GANs is their learning process. VAEs are minimizing a loss reproducing a certain\n",
        "image, and can, therefore, be considered as solving a semisupervised learning problem. GANs, on the other hand, are solving an unsupervised learning problem. <br>\n",
        "The most important difference found is the training time for the two methods. GANs took longer time to train. Therefore the use of GANs is considered and proved a lot more stable. With GANs this does not necessarily occur. For low-diversity datasets like MNIST, both methods give sufficiently realistic images. <br>\n",
        "Finally, using VAEs one can achieve results in less time, but with decreased image quality compared to results of GANs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJsVPWYeJaoG"
      },
      "source": [
        "**2. Familiarize yourself with the following type of GANS and briefly explain how each technique differs from vanilla GANs and give an application example for each type.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNKSHeNlJmb9"
      },
      "source": [
        "**a. Deep Convolutional GANs (DCGANs)**\n",
        "\n",
        "The deep convolutional generative adversarial network, or DCGAN for short, is an extension of the GAN architecture for using deep convolutional neural networks for both the generator and discriminator models and configurations for the models and training that result in the stable training of a generator model.\n",
        "\n",
        "The DCGAN is important because it suggested the constraints on the model required to effectively develop high-quality generator models in practice. This architecture, in turn, provided the basis for the rapid development of a large number of GAN extensions and applications.\n",
        "\n",
        "Application Example:\n",
        "\n",
        "Generate Cartoon Characters: training and use of a DCGAN for generating faces of anime characters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s92X4ZXXJtpW"
      },
      "source": [
        "**b. Wasserstein GANs (WGANs)**\n",
        "\n",
        "The Wasserstein generative adversarial network, or WGAN for short, is an extension to the GAN that changes the training procedure to update the discriminator model, now called a critic, many more times than the generator model for each iteration.\n",
        "\n",
        "The critic is updated to output a real-value (linear activation) instead of a binary prediction with a sigmoid activation, and the critic and generator models are both trained using “Wasserstein loss,” which is the average of the product of real and predicted values from the critic, designed to provide linear gradients that are useful for updating the model.\n",
        "\n",
        "In addition, the weights of the critic model are clipped to keep them small, e.g. a bounding box of $[-0.01. 0.01]$.\n",
        "\n",
        "In order to have parameters $w$ lie in a compact space, something simple we can do is clamp the weights to a fixed box (say $W = [−0.01, 0.01]$ ) after each gradient update.\n",
        "\n",
        "The benefit of the WGAN is that the training process is more stable and less sensitive to model architecture and choice of hyperparameter configurations. Perhaps most importantly, the loss of the discriminator appears to relate to the quality of images created by the generator.\n",
        "\n",
        "Application Example:\n",
        "\n",
        "Distinguish hand-written digits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qhx4RIOdJt1s"
      },
      "source": [
        "**c. Self-Attention GANs (SAGANs)**\n",
        "\n",
        "A Self-attention GAN is a DCGAN that utilizes self-attention layers. The idea of self-attention has been out there for years, also known as non-local in some researches. \n",
        "Convolution works by convolving nearby pixels and extracting features out of local blocks. They work “locally” in each layer. In contrast, self-attention layers learn from distant blocks.\n",
        "\n",
        "Application Example:\n",
        "\n",
        "To refine the image quality of the eye region (the red dot on the left figure), SAGAN only uses the feature map region on the highlight area in the middle figure. This region has a larger receptive field and the context is more focus and more relevant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF41syBlJumP"
      },
      "source": [
        "**d. BigGANs**\n",
        "\n",
        "The big generative adversarial network, or BigGAN for short, is an approach that demonstrates how high-quality output images can be created by scaling up existing class-conditional GAN models.\n",
        "\n",
        "The model architecture is based on a collection of best practices across a wide range of GAN models and extensions. Further improvements are achieved through systematic experimentation.\n",
        "\n",
        "A “truncation trick” is used where points are sampled from a truncated Gaussian latent space at generation time that is different from the untruncated distribution at training time.\n",
        "\n",
        "Application Example:\n",
        "\n",
        "Generate Realistic Photographs: generation of synthetic photographs with technique BigGAN that are practically indistinguishable from real photographs."
      ]
    }
  ]
}